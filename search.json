[
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "This comes from the file data.qmd.\nYour first steps in this project will be to find data to work on.\nI recommend trying to find data that interests you and that you are knowledgeable about. A bad example would be if you have no interest in video games but your data set is about video games. I also recommend finding data that is related to current events, social justice, and other areas that have an impact.\nInitially, you will study one dataset but later you will need to combine that data with another dataset. For this reason, I recommend finding data that has some date and/or location components. These types of data are conducive to interesting visualizations and analysis and you can also combine this data with other data that also has a date or location variable. Data from the census, weather data, economic data, are all relatively easy to combine with other data with time/location components."
  },
  {
    "objectID": "data.html#what-makes-a-good-data-set",
    "href": "data.html#what-makes-a-good-data-set",
    "title": "Data",
    "section": "What makes a good data set?",
    "text": "What makes a good data set?\n\nData you are interested in and care about.\nData where there are a lot of potential questions that you can explore.\nA data set that isn’t completely cleaned already.\nMultiple sources for data that you can combine.\nSome type of time and/or location component."
  },
  {
    "objectID": "data.html#where-to-keep-data",
    "href": "data.html#where-to-keep-data",
    "title": "Data",
    "section": "Where to keep data?",
    "text": "Where to keep data?\nBelow 50mb: In dataset folder\nAbove 50mb: In dataset-ignore folder which you will have to create manually. This folder will be ignored by git so you’ll have to manually sync these files across your team.\n\nSharing your data\nFor small datasets (&lt;50mb), you can use the dataset folder that is tracked by github. Stage and commit the files just like you would any other file.\nFor larger datasets, you’ll need to create a new folder in the project root directory named dataset-ignore. This will be ignored by git (based off the .gitignore file in the project root directory) which will help you avoid issues with Github’s size limits. Your team will have to manually make sure the data files in dataset-ignore are synced across team members.\nYour clean_data.R file in the scripts folder is the file where you will import the raw data that you download, clean it, and write .rds file(s) (using write_rds) that you’ll load in your analysis page. If desirable, you can have multiple scripts that produce different derived data sets, just make sure to link to them on this page.\nYou should never use absolute paths (eg. /Users/danielsussman/path/to/project/ or C:\\MA415\\\\Final_Project\\). Instead, use the here function from the here package to avoid path problems.\n\n\nClean data script\nThe idea behind this file is that someone coming to your website could largely replicate your analyses after running this script on the original data sets to clean them. This file might create a derivative data set that you then use for your subsequent analysis. Note that you don’t need to run this script from every post/page. Instead, you can load in the results of this script, which will usually be .rds files. In your data page you’ll describe how these results were created. If you have a very large data set, you might save smaller data sets that you can use for exploration purposes. To link to this file, you can use [cleaning script](/scripts/clean_data.R) wich appears as cleaning script."
  },
  {
    "objectID": "data.html#rubric-on-this-page",
    "href": "data.html#rubric-on-this-page",
    "title": "Data",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nDescribe where/how to find data.\n\nYou must include a link to the original data source(s). Make sure to provide attribution to those who collected the data.\nWhy was the data collected/curated? Who put it together? (This is important, if you don’t know why it was collected then that might not be a good dataset to look at.\n\nDescribe the different data files used and what each variable means.\n\nIf you have many variables then only describe the most relevant ones, possibly grouping together variables that are similar, and summarize the rest.\nUse figures or tables to help explain the data. For example, showing a histogram or bar chart for a particularly important variable can provide a quick overview of the values that variable tends to take.\n\nDescribe any cleaning you had to do for your data.\n\nYou must include a link to your clean_data.R file.\nRename variables and recode factors to make data more clear.\nAlso, describe any additional R packages you used outside of those covered in class.\nDescribe and show code for how you combined multiple data files and any cleaning that was necessary for that.\nSome repetition of what you do in your clean_data.R file is fine and encouraged if it helps explain what you did.\n\nOrganization, clarity, cleanliness of the page\n\nMake sure to remove excessive warnings, use clean easy-to-read code (without side scrolling), organize with sections, use bullets and other organization tools, etc.\nThis page should be self-contained."
  },
  {
    "objectID": "data.html#data-description",
    "href": "data.html#data-description",
    "title": "Data",
    "section": "Data Description",
    "text": "Data Description\nThe CalEnviroScreen data, available for download at “OEHHA’s website”, is compiled by the Office of Environmental Health Hazard Assessment (OEHHA) under the California Environmental Protection Agency (CalEPA). This dataset is designed to assess cumulative environmental burdens and population vulnerabilities across California’s communities. It includes indicators on pollution exposure, environmental effects, sensitive populations, and socioeconomic factors, allowing policymakers, researchers, and the public to identify areas most impacted by environmental hazards. The data was collected to support state efforts in environmental justice and resource allocation, particularly to assist in directing funding and policy initiatives to disadvantaged communities. By integrating environmental and demographic data, CalEnviroScreen provides a comprehensive tool for understanding disparities in environmental health risks across the state.\nThe data was collected to detect air pollution in small communities in California such as San Ysidro in San Diego. Since residents always complain that government air monitoring does not adequately measure air quality in their community. By collaborating with the San Ysidro community in San Diego, state and local government, and collecting data using low-cost technology, the San Ysidro Air Study group can give the rights to the general residents in San Ysidro for making decisions.\n\nWho put it together\nThe San Ysidro Air Study put the data together. But during the process of collecting data and selecting appropriate data, The San Ysidro Air Study group does corroborate with the local government, local groups(Casa Familiar), schools(University of Washington and San Diego State University), and The Community Steering Committee(including 12 volunteers from San Ysidro community)\n\n\nData Usage, Potential Similar Research, Analysis for Policy?\nThe CalEnviroScreen data has been used effectively in identifying the affected communities hence calling for policy action to direct cap-and-trade revenues to highly impacted areas, and also has been updated over the years to include more indicators and greater geographic specificity. This information is used by policymakers to allocate funds for environmental justice, guide regulatory enforcement, and support programs that target pollution control and public health improvements. The tool has also been applied to border regions under Assembly Bill 1059, which ensures pollution burdens within the context of environmental policies near the California-Mexico border.\nResearchers have used CalEnviroScreen to examine racial and socioeconomic disparities in environmental health risks. Cushing et al. (2015) found that pollution burdens disproportionately impact communities of color, particularly Hispanic and African American populations. Other studies, Alexeeff & Mataka (2014) and Meehan August et al. (2012), have explored methodological improvements and policy applications of the tool. Some of the key questions include how pollution exposure varies with race, how effective is the CalEnviroScreen and how can we improve it through additional indicators."
  },
  {
    "objectID": "data.html#data-files-and-description",
    "href": "data.html#data-files-and-description",
    "title": "Data",
    "section": "Data Files and Description",
    "text": "Data Files and Description\nThe original data was downloaded from CalEnviroScreen 4.0, which provides statewide data on environmental and demographic indicators at the census tract level. The data came in a single Excel file with multiple sheets. One sheet contained pollution and environmental exposure indicators, while another contained population and demographic characteristics. To prepare the data for analysis, we extracted each of these sheets and saved them as separate CSV files: pollution_data.csv and demographic_data.csv.\nThe pollution_data.csv file includes key variables such as Ozone, PM2.5, Diesel PM, and Traffic, along with their respective percentiles (e.g., “Ozone Pctl”, “PM2.5 Pctl”). These values quantify the level of environmental pollution affecting each census tract. It also includes summary indicators such as “Pollution Burden Score” and “CES 4.0 Score”, which are composite measures used by the state to assess environmental vulnerability.\nThe demographic_data.csv file contains population-level statistics, including Total Population, Children &lt; 10 years (%), and Elderly &gt; 64 years (%), as well as racial and ethnic breakdowns such as Hispanic (%), African American (%), and Asian American (%). These demographic variables allow for an assessment of how pollution levels intersect with age and race across regions.\nTo simplify the analysis, we focused on the most relevant environmental and demographic variables and removed columns related to less directly useful metrics like groundwater threats, education, and unemployment. Together, the cleaned and merged dataset allows us to explore the relationship between environmental burdens and community demographics in California.\n\nVariable description:\nEnvironmental Related statistics: like CES4.0 Score and PM 2.5 value, representing pollution score and levels.\nAge: Age of sample, ranging from children under 10 to elderly above 64\nLocation: California county that the census tract falls within\nRace: including races of samples in the research\nOther related statistics: Like poverty, education level, unemployment rate, Housing burden, Birth weight, potentially revealing relationships between those statistics and environmental levels. Offical Data Dictonary"
  },
  {
    "objectID": "data.html#data-loading-and-cleaning",
    "href": "data.html#data-loading-and-cleaning",
    "title": "Data",
    "section": "Data Loading and Cleaning",
    "text": "Data Loading and Cleaning\nMerging Pollution and Demographic Data\nFor this project, we worked with two datasets: pollution_data.csv and demographic_data.csv. Both datasets were read into R using the read_csv() function from the tidyverse package, and saved as .rds files using write_rds() for easier access later. The two datasets share some columns, including Total Population, California County, CES 4.0 Score, CES 4.0 Percentile, and CES 4.0 Percentile Range. So these columns were dropped from the pollution_data dataset before being merged with the demographic_data dataset. This was done using the following code:\n\npollution_data &lt;- pollution_data |&gt; \n                    select(-c('Total Population', 'California County', 'CES 4.0 Score', \n                              'CES 4.0 Percentile', 'CES 4.0 Percentile Range'))\n\nThe two datasets were then merged using a left join on the shared column “Census Tract” to ensure that all pollution records remained, even if corresponding demographic data was missing. This was done with the following line of code:\n\ncleaned_dataset &lt;- merge(pollution_data, demographic_data, by = \"Census Tract\", all.x = TRUE)\n\nRemoving Columns\nAfter merging, we removed several columns that were not relevant to our analysis. The original dataset included data for both air and water pollution. However, since our project is focused on air pollution we have removed the columns relevant to water pollution. These included environmental and social indicators such as drinking water, lead, pesticides, unemployment, and housing burden. Because the merge process introduced suffixes like .x and .y to distinguish duplicate column names, we used a for loop combined with select(-contains(…)) to remove all columns containing those key patterns, regardless of suffix:\n\ncolumns_to_remove_patterns &lt;- c(\n\"Drinking Water\", \"Lead\", \"Pesticides\", \"Groundwater Threats\", \"Imp. Water Bodies\", \"Education\", \"Linguistic Isolation\",\"Poverty\", \"Unemployment\", \"Housing Burden\"\n)\n\nfor (pattern in columns_to_remove_patterns) {\n  cleaned_dataset &lt;- cleaned_dataset %&gt;%\n    select(-contains(pattern))\n}\n\nRemoving Missing Values\nWe then checked for missing values using colSums(is.na(cleaned_dataset)) and found that the variable with the most missing data was related to low birth weight. Despite this, we chose to remove all rows containing any missing values in the dataset using the drop_na() function from the tidyr package:\n\ncleaned_dataset &lt;- cleaned_dataset %&gt;%\n  drop_na()\n\nWriting the Transformation to Clenaed Dataset\nFinally, the cleaned dataset was saved using:\n\nwrite_rds(cleaned_dataset, file = here::here(\"dataset\", \"cleaned_dataset.rds\"))\n\nAll steps were carried out using packages from the tidyverse, including dplyr for data manipulation, readr for reading in the datasets, tidyr for handling missing values, and here for managing file paths. No additional R packages beyond those covered in class were used. A full record of these operations is provided in the script: clean_data.R."
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Analysis",
    "section": "",
    "text": "This comes from the file analysis.qmd.\nWe describe here our detailed data analysis. This page will provide an overview of what questions you addressed, illustrations of relevant aspects of the data with tables and figures, and a statistical model that attempts to answer part of the question. You’ll also reflect on next steps and further analysis.\nThe audience for this page is someone like your class mates, so you can expect that they have some level of statistical and quantitative sophistication and understand ideas like linear and logistic regression, coefficients, confidence intervals, overfitting, etc.\nWhile the exact number of figures and tables will vary and depend on your analysis, you should target around 5 to 6. An overly long analysis could lead to losing points. If you want you can link back to your blog posts or create separate pages with more details.\nThe style of this paper should aim to be that of an academic paper. I don’t expect this to be of publication quality but you should keep that aim in mind. Avoid using “we” too frequently, for example “We also found that …”. Describe your methodology and your findings but don’t describe your whole process."
  },
  {
    "objectID": "analysis.html#note-on-attribution",
    "href": "analysis.html#note-on-attribution",
    "title": "Analysis",
    "section": "Note on Attribution",
    "text": "Note on Attribution\nIn general, you should try to provide links to relevant resources, especially those that helped you. You don’t have to link to every StackOverflow post you used but if there are explainers on aspects of the data or specific models that you found helpful, try to link to those. Also, try to link to other sources that might support (or refute) your analysis. These can just be regular hyperlinks. You don’t need a formal citation.\nIf you are directly quoting from a source, please make that clear. You can show long quotes using &gt; like this\n&gt; To be or not to be.\n\nTo be or not to be."
  },
  {
    "objectID": "analysis.html#rubric-on-this-page",
    "href": "analysis.html#rubric-on-this-page",
    "title": "Analysis",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nIntroduce what motivates your Data Analysis (DA)\n\nWhich variables and relationships are you most interested in?\nWhat questions are you interested in answering?\nProvide context for the rest of the page. This will include figures/tables that illustrate aspects of the data of your question.\n\nModeling and Inference\n\nThe page will include some kind of formal statistical model. This could be a linear regression, logistic regression, or another modeling framework.\nExplain the ideas and techniques you used to choose the predictors for your model. (Think about including interaction terms and other transformations of your variables.)\nDescribe the results of your modelling and make sure to give a sense of the uncertainty in your estimates and conclusions.\n\nExplain the flaws and limitations of your analysis\n\nAre there some assumptions that you needed to make that might not hold? Is there other data that would help to answer your questions?\n\nClarity Figures\n\nAre your figures/tables/results easy to read, informative, without problems like overplotting, hard-to-read labels, etc?\nEach figure should provide a key insight. Too many figures or other data summaries can detract from this. (While not a hard limit, around 5 total figures is probably a good target.)\nDefault lm output and plots are typically not acceptable.\n\nClarity of Explanations\n\nHow well do you explain each figure/result?\nDo you provide interpretations that suggest further analysis or explanations for observed phenomenon?\n\nOrganization and cleanliness.\n\nMake sure to remove excessive warnings, hide all code, organize with sections or multiple pages, use bullets, etc.\nThis page should be self-contained, i.e. provide a description of the relevant data.\n\n\n\nlibrary(tidyverse)\nlibrary(dplyr)\ndata &lt;- read_rds(here::here(\"dataset/cleaned_dataset.rds\"))\npollution_data &lt;- read_rds(here::here(\"dataset/pollution_data.rds\"))\ndemographic_data &lt;- read_rds(here::here(\"dataset/demographic_data.rds\"))\n\nThere are 8035 unique census tracts, 58 California counties, and 785 approximate city/town/areas in the dataset.\n\nn_distinct(unique(pollution_data$`Census Tract`))\n\n[1] 8035\n\nn_distinct(unique(pollution_data$`California County`))\n\n[1] 58\n\nn_distinct(unique(pollution_data$`Approximate Location`))\n\n[1] 785\n\n\nThere were 3390 missing values in the pollution dataset and 516 missing values in the demographic dataset. However, none of the pollution data is missing.\n\ncolSums(is.na(pollution_data))\n\n               Census Tract            Total Population \n                          0                           0 \n          California County                         ZIP \n                          0                           0 \n       Approximate Location                   Longitude \n                          0                           0 \n                   Latitude               CES 4.0 Score \n                          0                         103 \n         CES 4.0 Percentile    CES 4.0 Percentile Range \n                        103                         103 \n                      Ozone                  Ozone Pctl \n                          0                           0 \n                      PM2.5                  PM2.5 Pctl \n                          0                           0 \n                  Diesel PM              Diesel PM Pctl \n                          0                           0 \n             Drinking Water         Drinking Water Pctl \n                         28                          28 \n                       Lead                   Lead Pctl \n                         96                          96 \n                 Pesticides             Pesticides Pctl \n                          0                           0 \n               Tox. Release           Tox. Release Pctl \n                          0                           0 \n                    Traffic                Traffic Pctl \n                         35                          35 \n              Cleanup Sites          Cleanup Sites Pctl \n                          0                           0 \n        Groundwater Threats    Groundwater Threats Pctl \n                          0                           0 \n                 Haz. Waste             Haz. Waste Pctl \n                          0                           0 \n          Imp. Water Bodies      Imp. Water Bodies Pctl \n                          0                           0 \n                Solid Waste            Solid Waste Pctl \n                          0                           0 \n           Pollution Burden      Pollution Burden Score \n                          0                           0 \n      Pollution Burden Pctl                      Asthma \n                          0                          11 \n                Asthma Pctl            Low Birth Weight \n                         11                         227 \n      Low Birth Weight Pctl      Cardiovascular Disease \n                        227                          11 \nCardiovascular Disease Pctl                   Education \n                         11                         103 \n             Education Pctl        Linguistic Isolation \n                        103                         320 \n  Linguistic Isolation Pctl                     Poverty \n                        320                          75 \n               Poverty Pctl                Unemployment \n                         75                         335 \n          Unemployment Pctl              Housing Burden \n                        335                         145 \n        Housing Burden Pctl                  Pop. Char. \n                        145                         103 \n           Pop. Char. Score             Pop. Char. Pctl \n                        103                         103 \n\n\nThis bar chart plots the number of missing values for the column CES 4.0 Score by California County. The CalEnviroScreen Score is the Pollution Score multiplied by Population Characteristics Score. Los Angles is the county with the most missing values. We can examine through the demographic data to understand what potential communities were not captured by this lack of data. The code to angle the x labels was sourced from Substack\n\nmissing_counts &lt;- pollution_data |&gt;\n  group_by(`California County`) |&gt; \n  summarize(missing_count = sum(is.na(`CES 4.0 Score`))) |&gt;\n  filter(missing_count &gt; 0) |&gt;\n  arrange(missing_count)\n\n\nggplot(missing_counts, aes(x = `California County`, y = `missing_count`, fill = `missing_count`)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Missing Data Count by County\",\n       x = \"California County\",\n       y = \"Number of Missing Values\") +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n\n\n\n\n\n\n\ndemographic_data\n\n# A tibble: 8,035 × 15\n   `Census Tract` `CES 4.0 Score` `CES 4.0 Percentile` CES 4.0 Percentile Rang…¹\n            &lt;dbl&gt;           &lt;dbl&gt;                &lt;dbl&gt; &lt;chr&gt;                    \n 1     6001400100            4.85                 2.8  1-5% (lowest scores)     \n 2     6001400200            4.88                 2.87 1-5% (lowest scores)     \n 3     6001400300           11.2                 15.9  15-20%                   \n 4     6001400400           12.4                 19.0  15-20%                   \n 5     6001400500           16.7                 29.7  25-30%                   \n 6     6001400600           20.0                 37.6  35-40%                   \n 7     6001400700           36.7                 70.1  70-75%                   \n 8     6001400800           37.1                 70.7  70-75%                   \n 9     6001400900           40.7                 76.2  75-80%                   \n10     6001401000           43.7                 80.4  80-85%                   \n# ℹ 8,025 more rows\n# ℹ abbreviated name: ¹​`CES 4.0 Percentile Range`\n# ℹ 11 more variables: `California County` &lt;chr&gt;, `Total Population` &lt;dbl&gt;,\n#   `Children &lt; 10 years (%)` &lt;dbl&gt;, `Pop 10-64 years (%)` &lt;dbl&gt;,\n#   `Elderly &gt; 64 years (%)` &lt;dbl&gt;, `Hispanic (%)` &lt;dbl&gt;, `White (%)` &lt;dbl&gt;,\n#   `African American (%)` &lt;dbl&gt;, `Native American (%)` &lt;dbl&gt;,\n#   `Asian American (%)` &lt;dbl&gt;, `Other/Multiple (%)` &lt;dbl&gt;\n\n\nFor each census tract, races that are non-white tend to have smaller population percentages that are less than 25% of the census tract.\n\ndemo_long &lt;- demographic_data %&gt;%\n  pivot_longer(cols = c(`Hispanic (%)`, `White (%)`, `African American (%)`, `Native American (%)`,`Asian American (%)`,`Other/Multiple (%)`),\n               names_to = \"Race\",\n               values_to = \"Percentage\")\n\nggplot(demo_long, aes(x = Percentage)) +\n  geom_histogram(binwidth = 5) + \n  facet_wrap(~Race, scales = \"free_y\") + \n  labs(title = \"Distribution of Race Percentages\",\n       x = \"Percentage\",\n       y = \"Frequency\")\n\nWarning: Removed 138 rows containing non-finite outside the scale range\n(`stat_bin()`)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MA [46]15 Final Project",
    "section": "",
    "text": "Final Project due May 5, 2024 at 11:59pm.\nThis comes from the index.qmd file.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 3\n\n\nUpdates and Next Steps\n\n\n\n\n\n\n\n\nMar 24, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 2\n\n\n\n\n\n\n\n\n\n\n\nMar 17, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post 1\n\n\n\n\n\nblog post 1 \n\n\n\n\n\nMar 2, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nGeneral Tips\n\n\n\n\n\nSome small but important tips to follow. \n\n\n\n\n\nOct 4, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\n\n\n\n\n\n\nGetting started\n\n\n\n\n\n\n\n\nDirections to set up your website and create your first post. \n\n\n\n\n\nFeb 23, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\n\n\n\n\n\n\nFirst Team Meeting\n\n\n\n\n\n\n\n\nThis post details the steps you’ll take for your first team meeting. \n\n\n\n\n\nFeb 21, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-12-20-examples/examples.html",
    "href": "posts/2023-12-20-examples/examples.html",
    "title": "Examples",
    "section": "",
    "text": "Here are some examples of changing the size of a figure.\n\nplot(1:10)\n\n\n\n\n\n\n\n\n\nplot(1:10)\n\n\n\n\n\n\n\n\nWe can also specify column: screen and out-width: 100% so that the figure will fill the screen. plot in the svg vector graphics file format.\n\nlibrary(ggplot2)\nggplot(pressure, aes(x = temperature, y = pressure)) + geom_point()"
  },
  {
    "objectID": "posts/2023-12-20-examples/examples.html#figure-sizing",
    "href": "posts/2023-12-20-examples/examples.html#figure-sizing",
    "title": "Examples",
    "section": "",
    "text": "Here are some examples of changing the size of a figure.\n\nplot(1:10)\n\n\n\n\n\n\n\n\n\nplot(1:10)\n\n\n\n\n\n\n\n\nWe can also specify column: screen and out-width: 100% so that the figure will fill the screen. plot in the svg vector graphics file format.\n\nlibrary(ggplot2)\nggplot(pressure, aes(x = temperature, y = pressure)) + geom_point()"
  },
  {
    "objectID": "posts/2024-10-04-general-tips/general-tips.html",
    "href": "posts/2024-10-04-general-tips/general-tips.html",
    "title": "General Tips",
    "section": "",
    "text": "Use the tidyverse!\nYou don’t have to tell me what kind of chart something is. For example, the below is not a useful start to a sentence.\n\n\nThe graph presents a horizontal bar chart …\n\n\nEach page should be largely standalone.\nSometimes small tables or even inline numbers are better than a figure.\nRedundant colors (e.g. bar charts where each bar is a different color that doesn’t signify anything) often don’t help.\nProvide some details on how much data was removed in your cleaning process.\nUse the tidyverse!\nImagine I’m an impatient boss. Show me only what is important and relevant.\nCleaning must be entirely in R\nDon’t say things like, well if only everyone did like so and so than everything would be better. There are many things hiding behind the data that would go to explain things. This is an example of a bad conclusion.\n\n\nThe world could benefit form modeling its education systems after Europe’s.\n\nIt is fine to talk about how the European system is better according to certain metrics, but don’t assume that can easily translate to other regions.\n\nDon’t talk about your “journey”. The blog posts tell the story of your journey. The main pages should focus on the data and your findings.\n\n\nUse the tidyverse!\nNo but seriously, when asking ChatGPT to do your project for you, make sure to tell it to use the tidyverse, not base R."
  },
  {
    "objectID": "posts/2023-10-13-first-team-meeting/first-team-meeting.html",
    "href": "posts/2023-10-13-first-team-meeting/first-team-meeting.html",
    "title": "First Team Meeting",
    "section": "",
    "text": "These are the steps that you will take today to get started on your project. Today, you will just be brainstorming, and then next week, you’ll get started on the main aspects of the project.\n\nStart by introducing yourselves to each other. I also recommend creating a private channel on Microsoft Teams with all your team members. This will be a place that you can communicate and share ideas, code, problems, etc.\nDiscuss what aspects of the project each of you are more or less excited about. These include\n\nCollecting, cleaning, and munging data ,\nStatistical Modeling,\nVisualization,\nWriting about analyses, and\nManaging and reviewing team work.\n\nBased on this, discuss where you feel your strengths and weaknesses might be.\nNext, start brainstorming questions you hope to answer as part of this project. This question should in some way be addressing issues around racial disparities. The questions you come up with should be at the level of the question we started with when exploring the HMDA data. (“Are there differences in the ease of securing a loan based on the race of the applicant?”) You’ll revise your questions a lot over the course of the project. Come up with a few questions that your group might be interested in exploring.\nBased on these questions, start looking around for data that might help you analyze this. If you are looking at U.S. based data, data.gov is a good source and if you are looking internationally, I recommend checking out the World Bank. Also, try Googling for data. Include “data set” or “dataset” in your query. You might even include “CSV” or some other format. Using “data” by itself in your query often doesn’t work too well. Spend some time searching for data and try to come up with at least three possible data sets. (For your first blog post, you’ll write short proposals about each of them that I’ll give feedback on.)\nCome up with a team name. Next week, I’ll provide the Github Classroom assignment that will be where you work on your final project and you’ll have to have your team name finalized by then. Your project will be hosted online at the website with a URL like sussmanbu.github.io/ma4615-fa23-final-project-TEAMNAME.\n\nNext time, you’ll get your final project website set up and write your first blog post."
  },
  {
    "objectID": "posts/2023-10-15-getting-started/getting-started.html",
    "href": "posts/2023-10-15-getting-started/getting-started.html",
    "title": "Getting started",
    "section": "",
    "text": "Below, the items marked with [[OP]] should only be done by one person on the team.\n\nTo get started\n\n[[OP]] One person from the team should click the Github Classroom link on Teams.\n[[OP]] That person types in the group name for their group.\nThe rest of the team now clicks the Github Classroom link and selects their team from the dropdown list.\nFinally, each of you can clone the repository to your laptop like a normal assignment.\n\n\n\nSetting up the site\n\n[[OP]] Open the terminal and run quarto publish gh-pages.\n[[OP]] Select Yes to the prompt:  ? Publish site to https://sussmanbu.github.io/ma4615-fa23-final-project-TEAMNAME/ using gh-pages? (Y/n)\n[[OP]] Wait for the process to finish.\nOnce it is done, you can go to the URL it asked you about to see your site.\n\nNote: This is the process you will use every time you want to update your published site. Make sure to always follow the steps below for rendering, previewing, and committing your changes before doing these publish steps. Anyone can publish in the future.\n\n\nCustomize your site\n\n[[OP]] Open the _quarto.yml file and update the title to include your team name.\n[[OP]] Go to the about.qmd and remove the TF’s and professor’s names.\nadd your own along with a short introduction and a link to your Github user page.\n[[OP]] Render the site.\n[[OP]] Check and make sure you didn’t get any errors.\n[[OP]] Commit your changes and push.\n[[OP]] Repeat the steps under Setting up your site.\n\nOnce one person is done with this, each teammate in the group can, in turn, repeat steps 3-7. Before doing so, make sure to pull the changes from teammates before starting to make new changes. (We’ll talk soon about ways to organize your work and resolve conflicts.)\n\n\nStart your first post\n\nTo start your first post first, run remotes::install_github(\"sussmanbu/quartopost\") in your Console.\n[[OP]] Run quartopost::quartopost() (or click Addins-&gt;Create Quarto Post, or use C-Shift-P, type “Create Quarto” and press enter to run the command).\n\nNow you can start working on your post. You’ll want to render your post to see what it will look like on the site.\n\nEvery time you want to make a new post, you can repeat step 2 above.\nWhen you want to publish your progress, follow steps 4-7 from Customize your site.\n\nFinally, make sure to read through everything on this site which has the directions and rubric for the final project."
  },
  {
    "objectID": "posts/2025-03-17-blog-post-2/blog-post-2.html",
    "href": "posts/2025-03-17-blog-post-2/blog-post-2.html",
    "title": "Blog Post 2",
    "section": "",
    "text": "Our Progress\nOur team has officially decided to move forward with our project on disparities in air pollution exposure among racial and ethnic groups. We have made some progress on the Data page, working on the data description section. Our group has decided to split the parts of the rubric to complete the Data page. We are first working on each section of the page on a Google doc and will combine all of our work onto the qmd file later on.\nAs for the data, we are focused on integrating racial demographic information by joining our two datasets in the “Census Tract” column. This step allows us to analyze air pollution exposure in the context of racial and ethnic composition across different geographic areas. Currently, our data is separated into two files. One with information on air pollution and the other with demographic information. Once this integration is complete, our next steps will involve exploring and cleaning the data to address missing values, potential inconsistencies, and outliers. This will ensure the dataset is structured properly for in-depth analysis. In the coming weeks, we plan to conduct exploratory data analysis (EDA), visualizing trends and disparities to gain initial insights before diving into more advanced statistical and spatial analysis."
  },
  {
    "objectID": "posts/2025-03-02-blog-post-1/blog-post-1.html",
    "href": "posts/2025-03-02-blog-post-1/blog-post-1.html",
    "title": "Blog Post 1",
    "section": "",
    "text": "Dataset 1\nLink to Data Description: https://data.cityofnewyork.us/City-Government/Evictions/6z8x-wfk4/about_data\nThe New York City Department of Investigation updates a dataset daily on pending, scheduled and executed evictions within the five boroughs from the year 2017 to the present. The data is compiled from the majority of New York City Marshals, who are independent public officials. The dataset has 104,860 rows that each represent a pending, scheduled, or executed eviction. It also has 20 columns, which iIncludes attributes, such as Court Index Number, Docket Number, Eviction Address, Apartment Number, Executed Date, Marshal First Name, Marshal Last Name, Residential or Commercial (property type), Borough, Zip Code and Scheduled Status (Pending/Scheduled). We will be able to download the data as a csv file from the NYC Open Data website and potentially explore questions, such as the relationship between NYC housing eviction rate and neighborhood, what neighborhoods are more often subject to evictions, and if there any patterns between the ethnic makeup of the neighborhood with the number of evictions. It may be a challenge to make connections to racial disparities because the dataset itself does not have racial demographic data and this would have to be sourced based on the zip codes of where the evictions are taking place.\n\n\nDataset 2\nLink to Data Description: https://drive.google.com/file/d/1QRubjBCrkqe61iDkxFg7P_Lc1SkR2PhL/view?usp=sharing\nThe California Office of Environmental Health Hazard Assessment (OEHHA) developed a dataset with 8,035 census tracts and 58 columns to identify communities most burdened by pollution and vulnerable populations. It integrates environmental indicators (e.g., air pollution, drinking water contamination) and population characteristics (e.g., poverty, asthma rates, linguistic isolation) to guide environmental justice policies. Key research questions include whether communities with higher racial/ethnic minority populations face greater air pollution exposure and if specific racial/ethnic groups experience multiple pollution burdens disproportionately. A potential challenge is that the dataset only covers California, limiting generalizability to the U.S. Additionally, the racial/ethnic data is in a separate sheet, requiring a join on the Census Tract column for analysis. The data has been successfully loaded and is ready for cleaning.\n\n\nDataset 3\nLink to Data Description: https://catalog.data.gov/dataset/rates-and-trends-in-hypertension-related-cardiovascular-disease-mortality-among-us-ad-2000-2fdf2\nThis dataset documents rates and trends in local hypertension-related cardiovascular disease (CVD) death rates. The dataset contains 1103872 rows and 24 columns, including information on county (or county equivalent) estimates of hypertension-related CVD death rates in 2000-2019 and trends during two intervals (2000-2010, 2010-2019) by age group (ages 35–64 years, ages 65 years and older), race/ethnicity (non-Hispanic American Indian/Alaska Native, non-Hispanic Asian/Pacific Islander, non-Hispanic Black, Hispanic, non-Hispanic White), and sex (female, male). The rates and trends were estimated using a Bayesian spatiotemporal model and a smoothed over space, time, and demographic group. Rates are age-standardized in 10-year age groups using the 2010 US population."
  },
  {
    "objectID": "posts/2025-03-24-blog-post-3/blog-post-3.html",
    "href": "posts/2025-03-24-blog-post-3/blog-post-3.html",
    "title": "Blog Post 3",
    "section": "",
    "text": "Our Progress\nOver the past week, we made substantial progress in preparing our dataset for analysis. The original data came from the CalEnviroScreen 4.0 Excel file, which included multiple sheets—one containing pollution and environmental exposure data, and another with demographic indicators. We exported each sheet into separate CSV files, resulting in pollution_data.csv and demographic_data.csv. These were then read into R using read_csv() and merged using a left join on the “Census Tract” column to retain all pollution records while appending demographic information wherever available. After the merge, we removed several columns that were not directly relevant to our analysis, such as “Drinking Water”, “Groundwater Threats”, “Unemployment”, and “Housing Burden”, using a loop with select(-contains(…)). This approach allowed us to cleanly remove both the original and duplicated columns (e.g., .x, .y suffixes) introduced during merging.\nWe also addressed missing values by running colSums(is.na(cleaned_dataset)) to identify which variables had the most missing data. After reviewing the results—where “Low Birth Weight” had the highest number of NA values—we chose to drop all rows with missing values using drop_na() to ensure a clean dataset moving forward.\nLooking ahead, we plan to begin our exploratory data analysis by visualizing how pollution exposure correlates with demographic indicators such as race, age, and income. We are also beginning to consider additional data sources that could enrich our analysis—such as housing data, health outcome statistics, or climate-related variables—to explore deeper patterns of environmental injustice and vulnerability across California communities.\n\n\nData for Equity\nThe beneficence principle applies where we have to be transparent about the limits of the data when processing and analyzing our data because we can only draw a limited number of conclusions using only the columns we have. Despite any wishes we may have to extend arguments to what causes the air pollution in some neighborhoods compared to others, our dataset only focuses on the pollution and health impacts without collecting data on how the neighborhoods may be developed infrastructurally to affect these outcomes. Regardless of what analytics we produce, we don’t have the data to make claims that the increased pollution is directly caused by any of the variables we have. In addition, we should also be clear about how any previous analysis done on the dataset informed our own analysis.\nAnother principle that applies is justice. When conceiving the idea of the project, we should be clear on what the analysis would contribute to the communities it would affect. Although we may not be able to hold listening communities, we as analysts can take the responsibility to research more on the background of how air pollution can negatively impact vulnerable communities in California, and also understand the current state of what is being done to try and address the issue. For this dataset, there could be potential abuse or misuse if we try to conclude more about what causes the increased pollution, which would be not informed by the data we have available. Also, by understanding if communities are disproportionately impacted by air pollution, there is potential that the analysis could be used to support projects that push for developments in these neighborhoods that may not necessarily be wanted by the community."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This comes from the file about.qmd.\nThis is a website for the final project for MA[46]15 Data Science with R by Team TEAMNAME. The members of this team are below."
  },
  {
    "objectID": "about.html#hannah-choe",
    "href": "about.html#hannah-choe",
    "title": "About",
    "section": "Hannah Choe",
    "text": "Hannah Choe\nHannah is a junior majoring in Data Science and minoring in Business Administration Github"
  },
  {
    "objectID": "about.html#rachel-young",
    "href": "about.html#rachel-young",
    "title": "About",
    "section": "Rachel Young",
    "text": "Rachel Young\nRachel is a senior studying Data Science. Github"
  },
  {
    "objectID": "about.html#thao-thiem",
    "href": "about.html#thao-thiem",
    "title": "About",
    "section": "Thao Thiem",
    "text": "Thao Thiem\nThao is a junior studying BA/MA in Econ and Maths. Github"
  },
  {
    "objectID": "about.html#wenbo-huang",
    "href": "about.html#wenbo-huang",
    "title": "About",
    "section": "Wenbo Huang",
    "text": "Wenbo Huang\nWenbo is a junior studying math and stats. Github"
  },
  {
    "objectID": "about.html#cuichen-li",
    "href": "about.html#cuichen-li",
    "title": "About",
    "section": "Cuichen Li",
    "text": "Cuichen Li\nCuichen is a junior, studying stats and econ. Github"
  },
  {
    "objectID": "about.html#section",
    "href": "about.html#section",
    "title": "About",
    "section": "",
    "text": "About this Template.\nThis is based off of the standard Quarto website template from RStudio (2023.09.0 Build 463)."
  },
  {
    "objectID": "big_picture.html",
    "href": "big_picture.html",
    "title": "Big Picture",
    "section": "",
    "text": "This comes from the file big_picture.qmd.\nThink of this page as your 538/Upshot style article. This means that you should try to tell a story through the data and your analysis. Read articles from those sites and similar sites to get a feeling for what they are like. Try to write in the style of a news or popular article. Importantly, this page should be geared towards the general public. You shouldn’t assume the reader understands how to interpret a linear regression or a complicated plot. Focus on interpretation and visualizations."
  },
  {
    "objectID": "big_picture.html#rubric-on-this-page",
    "href": "big_picture.html#rubric-on-this-page",
    "title": "Big Picture",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\n\nTitle\n\nYour big picture page should have a creative/click-bait-y title/headline that provides a hint about your thesis.\n\nClarity of Explanation\n\nYou should have a clear thesis/goal for this page. What are you trying to show? Make sure that you explain your analysis in detail but don’t go into top much mathematics or statistics. The audience for this page is the general public (to the extent possible). Your thesis should be a statement, not a question.\nEach figure should be very polished and also not too complicated. There should be a clear interpretation of the figure so the figure has a clear purpose. Even something like a histogram can be difficult to interpret for non-experts.\n\nCreativity\n\nDo your best to make things interesting. Think of a how a news article or a magazine story might draw you in. Think of how each part of your analysis supports the previous part or provides a different perspective.\n\nInteractive component\n\nQuality and ease of use of the interactive components. Is it clear what can be explored using your interactive components? Does it enhance and reinforce your conclusions?\n\nThis page should be self-contained.\n\nNote: This page should have no code visible, i.e. use #| echo: FALSE."
  },
  {
    "objectID": "big_picture.html#rubric-other-components",
    "href": "big_picture.html#rubric-other-components",
    "title": "Big Picture",
    "section": "Rubric: Other components",
    "text": "Rubric: Other components\n\nVideo Recording\nMake a video recording (probably using Zoom) demonstrating your interactive components. You should provide a quick explanation of your data and demonstrate some of the conclusions from your EDA. This video should be no longer than 4 minutes. Include a link to your video (and password if needed) in your README.md file on your Github repository. You are not required to provide a link on the website. This can be presented by any subset of the team members.\n\n\nRest of the Site\nFinally, here are important things to keep in mind for the rest of the site.\nThe main title of your page is informative. Each post has an author/description/informative title. All lab required posts are present. Each page (including the home page) has a nice featured image associated with it. Your about page is up to date and clean. You have removed the generic posts from the initial site template."
  }
]